---
layout: post
title: "celery 笔记\_"
tagline: 架构
category: null
tags: []
published: true

---
## 架构 笔记##

##网络通信协议 ##
诸如TCP/UDP等等）、网络IO（Blocking-IO，NonBlocking-IO、Asyn-IO）、网卡（多队列等）；更偏应用的层面，需要了解例如连接复用、序列化/反序列化、RPC、负载均衡等

##支持大量连接、高并发、低资源消耗的通信程序 ##
通信模式,中间件通讯模式,订阅等


1. 大量client连一个server
在现如今NonBlocking-IO这么成熟的情况下，一个支持大量client的server已经不那么难写了，但在大规模，并且通常长连接的情况下，有一个点要特别注意，就是当server挂掉的时候，不能出现所有client都在一个时间点发起重连，那样基本就是灾难，在没有经验的情况下我看过好几起类似的case，到client规模上去后，server一重启基本就直接被冲进来的大量建连冲垮了（当然，server的backlog队列首先应该稍微设置大一些），通常可以采用的方法是client重连前都做随机时间的sleep，另外就是重连的间隔采取避让算法。

2. 一个client连大量的server
有些场景也会出现需要连大量server的现象，在这种情况下，同样要注意的也是不要并发同时去建所有的连接，而是在能力范围内分批去建。
除了建连接外，另外还要注意的地方是并发发送请求也同样，一定要做好限流，否则很容易会因为一些点慢导致内存爆掉。

##高并发##
高并发这个点需要掌握CAS、常见的lock-free算法、读写锁、线程相关知识（例如线程交互、线程池）等，通信层面的高并发在NonBlocking-IO的情况下，最重要的是要注意在整体设计和代码实现上尽量减少对io线程池的时间占用。

##伸缩性##

分布式系统基本就意味着规模不小了，对于这类系统在设计的时候必须考虑伸缩性问题，架构图上画的任何一个点，如果请求量或者是数据量不断增大，怎么做到可以通过加机器的方式来解决，当然，这个过程也不用考虑无限大的场景，如果经历过从比较小到非常大规模的架构师，显然优势是不小的，同样也会是越来越稀缺的。

1. 无状态场景
对于无状态场景，要实现随量增长而加机器支撑会比较简单，这种情况下只用解决节点发现的问题，通常只要基于负载均衡就可以搞定，硬件或软件方式都有；
无状态场景通常会把很多状态放在db，当量到一定阶段后会需要引入服务化，去缓解对db连接数太多的情况。
2. 有状态场景
所谓状态其实就是数据，通常采用Sharding来实现伸缩性，Sharding有多种的实现方式，常见的有这么一些：
2.1 规则Sharding
基于一定规则把状态数据进行Sharding，例如分库分表很多时候采用的就是这样的，这种方式支持了伸缩性，但通常也带来了很复杂的管理、状态数据搬迁，甚至业务功能很难实现的问题，例如全局join，跨表事务等。
2.2 一致性Hash
一致性Hash方案会使得加机器代价更低一些，另外就是压力可以更为均衡，例如分布式cache经常采用，和规则Sharding带来的问题基本一样。
2.3 Auto Sharding
Auto Sharding的好处是基本上不用管数据搬迁，而且随着量上涨加机器就OK，但通常Auto Sharding的情况下对如何使用会有比较高的要求，而这个通常也就会造成一些限制，这种方案例如HBase。
2.4 Copy
Copy这种常见于读远多于写的情况，实现起来又会有最终一致的方案和全局一致的方案，最终一致的多数可通过消息机制等，全局一致的例如zookeeper/etcd之类的，既要全局一致又要做到很高的写支撑能力就很难实现了。

##稳定性##
作为分布式系统，必须要考虑清楚整个系统中任何一个点挂掉应该怎么处理（到了一定机器规模，每天挂掉一些机器很正常），同样主要还是分成了无状态和有状态：
1. 无状态场景
对于无状态场景，通常好办，只用节点发现的机制上具备心跳等检测机制就OK，经验上来说无非就是纯粹靠4层的检测对业务不太够，通常得做成7层的，当然，做成7层的就得处理好规模大了后的问题。
2. 有状态场景
对于有状态场景，就比较麻烦了，对数据一致性要求不高的还OK，主备类型的方案基本也可以用，当然，主备方案要做的很好也非常不容易，有各种各样的方案，对于主备方案又觉得不太爽的情况下，例如HBase这样的，就意味着挂掉一台，另外一台接管的话是需要一定时间的，这个对可用性还是有一定影响的；
全局一致类型的场景中，如果一台挂了，就通常意味着得有选举机制来决定其他机器哪台成为主，常见的例如基于paxos的实现。

##可维护性##
维护性是很容易被遗漏的部分，但对分布式系统来说其实是很重要的部分，例如整个系统环境应该怎么搭建，部署，配套的维护工具、监控点、报警点、问题定位、问题处理策略等等。


### 缓存 ###
1 动静分离
- 静态页面缓存在客户端,刷新到 cdn
- 动态数据分层效验,前端分流效验数据时候能够走 cache, 后端限流写数据任务
- 数据库层需要做强一致性效验

2 优化
- 建立实时热点分析系统,实时推送热点到 cache
- 请求在代理层上直接返回,不要下放到应用层.应用层只处理动态数据.

3 大并发
- 并发数据,包括动态静态全部进缓存
- 排队

###Nginx ###
![nginx](http://www.rowkey.me/images/blog_images/nginx/ngx_arch.jpg)
* 一个 master 进程负责分配工作,master进程主要用来管理worker进程


worker进程则是处理基本的网络事件。多个worker进程之间是对等的，他们同等竞争来自客户端的请求，各进程互相之间是独立的。一个请求，只可能在一个worker进程中处理，一个worker进程，不可能处理其它进程的请求。

开发模型：epoll和kqueue。

支持的事件机制：kqueue、epoll、rt signals、/dev/poll 、event ports、select以及poll。

1.1 负载均衡

* 加权均衡
![weight](http://www.rowkey.me/images/blog_images/nginx/ngx_wr.png)
排序 -> 链接 -> 减重 ->排序


* iphash

'''
 for(i = 0;i < 3;i++){
     hash = (hash * 113 + iphp->addr[i]) % 6271; 
 }

 p = hash % iphp->rrp.peers->number; 
'''

hash 值与 ip 和后端机器数量有关,最大1045个值.当经过20次hash仍然找不到可用的机器时，算法退化成轮询。如果两个ip的初始hash值恰好相同，那么来自这两个ip的请求将永远落在同一台服务器上，这为均衡性埋下了很深的隐患。

* fair

是根据后端服务器的响应时间判断负载情况，从中选出负载最轻的机器进行分流.

* 通用hash、一致性hash


* session_sticky
一次会话内的请求都会落到同一个结点上

1.2 动态负载均衡

1.2.1 自身监控


内置了对后端服务器的健康检查功能。如果Nginx proxy后端的某台服务器宕机了，会把返回错误的请求重新提交到另一个节点，不会影响前端访问。它没有独立的健康检查模块，而是使用业务请求作为健康检查，这省去了独立健康检查线程，这是好处。坏处是，当业务复杂时，可能出现误判，例如后端响应超时，这可能是后端宕机，也可能是某个业务请求自身出现问题，跟后端无关。




