---
layout: post
title: "celery 笔记\_"
tagline: 架构
category: null
tags: []
published: true

---
## 架构 笔记##

##网络通信协议 ##
诸如TCP/UDP等等）、网络IO（Blocking-IO，NonBlocking-IO、Asyn-IO）、网卡（多队列等）；更偏应用的层面，需要了解例如连接复用、序列化/反序列化、RPC、负载均衡等

##支持大量连接、高并发、低资源消耗的通信程序 ##
通信模式,中间件通讯模式,订阅等


1. 大量client连一个server
在现如今NonBlocking-IO这么成熟的情况下，一个支持大量client的server已经不那么难写了，但在大规模，并且通常长连接的情况下，有一个点要特别注意，就是当server挂掉的时候，不能出现所有client都在一个时间点发起重连，那样基本就是灾难，在没有经验的情况下我看过好几起类似的case，到client规模上去后，server一重启基本就直接被冲进来的大量建连冲垮了（当然，server的backlog队列首先应该稍微设置大一些），通常可以采用的方法是client重连前都做随机时间的sleep，另外就是重连的间隔采取避让算法。

2. 一个client连大量的server
有些场景也会出现需要连大量server的现象，在这种情况下，同样要注意的也是不要并发同时去建所有的连接，而是在能力范围内分批去建。
除了建连接外，另外还要注意的地方是并发发送请求也同样，一定要做好限流，否则很容易会因为一些点慢导致内存爆掉。

##高并发##
高并发这个点需要掌握CAS、常见的lock-free算法、读写锁、线程相关知识（例如线程交互、线程池）等，通信层面的高并发在NonBlocking-IO的情况下，最重要的是要注意在整体设计和代码实现上尽量减少对io线程池的时间占用。

##伸缩性##

分布式系统基本就意味着规模不小了，对于这类系统在设计的时候必须考虑伸缩性问题，架构图上画的任何一个点，如果请求量或者是数据量不断增大，怎么做到可以通过加机器的方式来解决，当然，这个过程也不用考虑无限大的场景，如果经历过从比较小到非常大规模的架构师，显然优势是不小的，同样也会是越来越稀缺的。

1. 无状态场景
对于无状态场景，要实现随量增长而加机器支撑会比较简单，这种情况下只用解决节点发现的问题，通常只要基于负载均衡就可以搞定，硬件或软件方式都有；
无状态场景通常会把很多状态放在db，当量到一定阶段后会需要引入服务化，去缓解对db连接数太多的情况。
2. 有状态场景
所谓状态其实就是数据，通常采用Sharding来实现伸缩性，Sharding有多种的实现方式，常见的有这么一些：
2.1 规则Sharding
基于一定规则把状态数据进行Sharding，例如分库分表很多时候采用的就是这样的，这种方式支持了伸缩性，但通常也带来了很复杂的管理、状态数据搬迁，甚至业务功能很难实现的问题，例如全局join，跨表事务等。
2.2 一致性Hash
一致性Hash方案会使得加机器代价更低一些，另外就是压力可以更为均衡，例如分布式cache经常采用，和规则Sharding带来的问题基本一样。
2.3 Auto Sharding
Auto Sharding的好处是基本上不用管数据搬迁，而且随着量上涨加机器就OK，但通常Auto Sharding的情况下对如何使用会有比较高的要求，而这个通常也就会造成一些限制，这种方案例如HBase。
2.4 Copy
Copy这种常见于读远多于写的情况，实现起来又会有最终一致的方案和全局一致的方案，最终一致的多数可通过消息机制等，全局一致的例如zookeeper/etcd之类的，既要全局一致又要做到很高的写支撑能力就很难实现了。

##稳定性##
作为分布式系统，必须要考虑清楚整个系统中任何一个点挂掉应该怎么处理（到了一定机器规模，每天挂掉一些机器很正常），同样主要还是分成了无状态和有状态：
1. 无状态场景
对于无状态场景，通常好办，只用节点发现的机制上具备心跳等检测机制就OK，经验上来说无非就是纯粹靠4层的检测对业务不太够，通常得做成7层的，当然，做成7层的就得处理好规模大了后的问题。
2. 有状态场景
对于有状态场景，就比较麻烦了，对数据一致性要求不高的还OK，主备类型的方案基本也可以用，当然，主备方案要做的很好也非常不容易，有各种各样的方案，对于主备方案又觉得不太爽的情况下，例如HBase这样的，就意味着挂掉一台，另外一台接管的话是需要一定时间的，这个对可用性还是有一定影响的；
全局一致类型的场景中，如果一台挂了，就通常意味着得有选举机制来决定其他机器哪台成为主，常见的例如基于paxos的实现。

##可维护性##
维护性是很容易被遗漏的部分，但对分布式系统来说其实是很重要的部分，例如整个系统环境应该怎么搭建，部署，配套的维护工具、监控点、报警点、问题定位、问题处理策略等等。


### 缓存 ###
1 动静分离
- 静态页面缓存在客户端,刷新到 cdn
- 动态数据分层效验,前端分流效验数据时候能够走 cache, 后端限流写数据任务
- 数据库层需要做强一致性效验

2 优化
- 建立实时热点分析系统,实时推送热点到 cache
- 请求在代理层上直接返回,不要下放到应用层.应用层只处理动态数据.

3 大并发
- 并发数据,包括动态静态全部进缓存
- 应用层对队列进行排序,避免同一资源请求占用太多连接
- 数据库层进行事务排队

###负载均衡###

####IP####

在网络层通过修改请求目标地址进行负载均衡。 用户请求数据包，到达负载均衡服务器后，负载均衡服务器在操作系统内核进程获取网络数据包，根据负载均衡算法得到一台真实服务器地址，然后将请求目的地址修改为，获得的真实ip地址，不需要经过用户进程处理。 真实服务器处理完成后，响应数据包回到负载均衡服务器，负载均衡服务器，再将数据包源地址修改为自身的ip地址，发送给用户浏览器。



####链路层####

在通信协议的数据链路层修改mac地址，进行负载均衡。 数据分发时，不修改ip地址，指修改目标mac地址，配置真实物理服务器集群所有机器虚拟ip和负载均衡服务器ip地址一致，达到不修改数据包的源地址和目标地址，进行数据分发的目的。 实际处理服务器ip和数据请求目的ip一致，不需要经过负载均衡服务器进行地址转换，可将响应数据包直接返回给用户浏览器，避免负载均衡服务器网卡带宽成为瓶颈。也称为直接路由模式（DR模式）。

####混合####　　

其实这就显而易见了，当单一的负载均衡方式无法很好的解决现有的问题，那么我们就可以把他们结合在一起使用，这也很符合当下的发展潮流啊… 具体的结合方式有很多，例如　我们可以考虑分层，在每一层采用不同的方式来进行负载均衡，在最外层使用 DNS负载均衡，在使用反向代理来做缓存以及动态请求分发 ，最后在是应用负载均衡(IP/DR), 分流到对应的应用集群　
![dr1](http://7xnp02.com1.z0.glb.clouddn.com/820332-20151213200106747-94797427.png)
![dr2](http://7xnp02.com1.z0.glb.clouddn.com/820332-20151213200117825-1452672107.png)


####LVS####

抗负载能力强，因为lvs工作方式的逻辑是非常之简单，而且工作在网络4层仅做请求分发之用，没有流量，所以在效率上基本不需要太过考虑。
配置性低，这通常是一大劣势，但同时也是一大优势，因为没有太多可配置的选项，所以除了增减服务器，并不需要经常去触碰它，大大减少了人为出错的几率。
工作稳定，因为其本身抗负载能力很强，所以稳定性高也是顺理成章，另外各种lvs都有完整的双机热备方案，所以一点不用担心均衡器本身会出什么问题，节点出现故障的话，lvs会自动判别，所以系统整体是非常稳定的。
无流量，上面已经有所提及了。lvs仅仅分发请求，而流量并不从它本身出去，所以可以利用它这点来做一些线路分流之用。没有流量同时也保住了均衡器的IO性能不会受到大流量的影响。
基本上能支持所有应用，因为lvs工作在4层，所以它可以对几乎所有应用做负载均衡，包括http、数据库、聊天室等等.

####ngingx haproxy ####

　
HaProxy

HAProxy是工作在网络7层之上。
能够补充Nginx的一些缺点比如Session的保持，Cookie的引导等工作
支持url检测后端的服务器出问题的检测会有很好的帮助。
更多的负载均衡策略比如：动态加权轮循(Dynamic Round Robin)，加权源地址哈希(Weighted Source Hash)，加权URL哈希和加权参数哈希(Weighted Parameter Hash)已经实现
单纯从效率上来讲HAProxy更会比Nginx有更出色的负载均衡速度。
HAProxy可以对Mysql进行负载均衡，对后端的DB节点进行检测和负载均衡。
Nignx

工作在网络的7层之上，可以针对http应用做一些分流的策略，比如针对域名、目录结构；
Nginx对网络的依赖比较小；
Nginx安装和配置比较简单，测试起来比较方便；
也可以承担高的负载压力且稳定，一般能支撑超过1万次的并发；
Nginx可以通过端口检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点，不过其中缺点就是不支持url来检测；
Nginx对请求的异步处理可以帮助节点服务器减轻负载；
Nginx能支持http和Email，这样就在适用范围上面小很多；
不支持Session的保持、对Big request header的支持不是很好，另外默认的只有Round-robin和IP-hash两种负载均衡算法。



###Nginx ###
![nginx](http://www.rowkey.me/images/blog_images/nginx/ngx_arch.jpg)
* 一个 master 进程负责分配工作,master进程主要用来管理worker进程


worker进程则是处理基本的网络事件。多个worker进程之间是对等的，他们同等竞争来自客户端的请求，各进程互相之间是独立的。一个请求，只可能在一个worker进程中处理，一个worker进程，不可能处理其它进程的请求。

开发模型：epoll和kqueue。

支持的事件机制：kqueue、epoll、rt signals、/dev/poll 、event ports、select以及poll。

1.1 负载均衡

* 加权均衡
![weight](http://www.rowkey.me/images/blog_images/nginx/ngx_wr.png)
排序 -> 链接 -> 减重 ->排序


* iphash

'''
 for(i = 0;i < 3;i++){
     hash = (hash * 113 + iphp->addr[i]) % 6271; 
 }

 p = hash % iphp->rrp.peers->number; 
'''

hash 值与 ip 和后端机器数量有关,最大1045个值.当经过20次hash仍然找不到可用的机器时，算法退化成轮询。如果两个ip的初始hash值恰好相同，那么来自这两个ip的请求将永远落在同一台服务器上，这为均衡性埋下了很深的隐患。

* fair

是根据后端服务器的响应时间判断负载情况，从中选出负载最轻的机器进行分流.

* 通用hash、一致性hash


* session_sticky
一次会话内的请求都会落到同一个结点上

1.2 动态负载均衡

1.2.1 自身监控


内置了对后端服务器的健康检查功能。如果Nginx proxy后端的某台服务器宕机了，会把返回错误的请求重新提交到另一个节点，不会影响前端访问。它没有独立的健康检查模块，而是使用业务请求作为健康检查，这省去了独立健康检查线程，这是好处。坏处是，当业务复杂时，可能出现误判，例如后端响应超时，这可能是后端宕机，也可能是某个业务请求自身出现问题，跟后端无关。


## 持续集成 ##


开发者更新工作区
Jenkins收到通知
Jenkins克隆工作区
Jenkins创建一个Docker镜像
Jenkins运行测试
Jenkins将镜像推到Docker Hub


